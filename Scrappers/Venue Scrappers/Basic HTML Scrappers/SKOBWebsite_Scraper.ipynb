{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import time as Time\n",
    "from posixpath import commonpath, splitext\n",
    "from bs4.element import NavigableString\n",
    "from dateutil.parser import parse\n",
    "from sys import excepthook\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import uuid\n",
    "import re\n",
    "from requests import exceptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next decide which urls to scarpe and put them into an array so you can iterate through them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#URL's to be scrapped\n",
    "url1 = 'https://www.skob.com/live-music/band-calendar'\n",
    "url2 = ''\n",
    "url3 = ''\n",
    "url4 = ''\n",
    "url5 = ''\n",
    "url_array = [url1, url2, url3, url4, url5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will iterate through the url_array and scrape webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Properties\n",
    "venue_name = 'Siesta Key Oyster Bar'\n",
    "band_name = ''\n",
    "date_string = ''\n",
    "\n",
    "ex_array = [\n",
    "    'GAME'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url1).text\n",
    "soup = BeautifulSoup(response, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = soup.find_all(class_ = 'djev_item_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events:\n",
    "    raw_band_name = event.find('h4').text.strip()\n",
    "\n",
    "    '''!!! FILTER!!! '''\n",
    "    if any([x in raw_band_name for x in ex_array]): \n",
    "        continue\n",
    "    \n",
    "    pattern = '\\d+'\n",
    "    numbers = re.findall(pattern, raw_band_name)\n",
    "    band_name = raw_band_name.split(f' {numbers[0]}')[0]\n",
    "\n",
    "    raw_date = event.find(class_=\"djev_time_from\").text.strip() + f' {numbers[0]}pm'\n",
    "\n",
    "    date = parse(raw_date)\n",
    "    date_string = '{:%b %d, %Y %-I:%M%p}'.format(date)\n",
    "\n",
    "    #Create model and add to array\n",
    "    showDict = {}\n",
    "    showDict['band'] = band_name\n",
    "    showDict['dateString'] = date_string\n",
    "    array.append(showDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scape_urls(arr):\n",
    "    for i in arr:\n",
    "        if i == '':\n",
    "            continue\n",
    "        \n",
    "        response = requests.get(i).text\n",
    "        soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "        for event in events:\n",
    "            raw_band_name = event.find('h4').text.strip()\n",
    "\n",
    "            '''!!! FILTER!!! '''\n",
    "            if any([x in raw_band_name for x in ex_array]): \n",
    "                continue\n",
    "\n",
    "            pattern = '\\d+'\n",
    "            numbers = re.findall(pattern, raw_band_name)\n",
    "            band_name = raw_band_name.split(f' {numbers[0]}')[0]\n",
    "\n",
    "            raw_date = event.find(class_=\"djev_time_from\").text.strip() + f' {numbers[0]}pm'\n",
    "\n",
    "            date = parse(raw_date)\n",
    "            date_string = '{:%b %d, %Y %-I:%M%p}'.format(date)\n",
    "\n",
    "            #Create model and add to array\n",
    "            showDict = {}\n",
    "            showDict['band'] = band_name\n",
    "            showDict['dateString'] = date_string\n",
    "            array.append(showDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = []\n",
    "scape_urls(url_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "venDict = {}\n",
    "venDict['venueName'] = venue_name\n",
    "venDict['shows'] = array\n",
    "\n",
    "venue_array = [venDict]\n",
    "finalDict = {}\n",
    "finalDict['venue'] = venue_array\n",
    "\n",
    "#Save To json file\n",
    "save_path = '/Users/nathanhedgeman/Documents/Scrappers/Show Data'\n",
    "file_name = venue_name + ' Website' + '.json'\n",
    "complete_name = os.path.join(save_path, file_name)\n",
    "\n",
    "file = open(complete_name, 'w')\n",
    "file.write(json.dumps(finalDict, indent = 2))\n",
    "file.close()\n",
    "shows = []\n",
    "print(\"Complete!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('python@3.9')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
