{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import time as Time\n",
    "from posixpath import commonpath, splitext\n",
    "from bs4.element import NavigableString\n",
    "from dateutil.parser import parse\n",
    "from sys import excepthook\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import uuid\n",
    "import re\n",
    "from requests import exceptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next decide which urls to scrape and put them into an array so you can iterate through them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#URL's to be scrapped\n",
    "url1 = 'https://flutendram.com/events/'\n",
    "url2 = ''\n",
    "url3 = ''\n",
    "url4 = ''\n",
    "url5 = ''\n",
    "url_array = [url1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will iterate through the url_array and scrape webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Properties\n",
    "venue_name = \"Flute & Dram\"\n",
    "band_name = ''\n",
    "date_string = ''\n",
    "\n",
    "ex_array = [\n",
    "    'Party', 'Fundraiser', 'Holiday', 'Christmas', 'New Years'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scape_urls(arr):\n",
    "    for i in arr:\n",
    "        response = requests.get(i).text\n",
    "        soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "        for single_Show in soup.find_all('div', class_='stevent-wrap'):\n",
    "\n",
    "            try:\n",
    "                #Grab its information and organize it\n",
    "                #Date\n",
    "                date_Div = single_Show.find('div', class_ = 'stevent-date')\n",
    "                month = date_Div.find('div', class_ = 'date-top')\n",
    "                day = date_Div.find('div', class_ = 'date-middle')\n",
    "                year = datetime.date.year\n",
    "                month_And_Day_And_Year = month.text + \" \" + day.text + \",\" + \" \" + datetime.datetime.now().strftime(\"%Y\")\n",
    "\n",
    "                #Band\n",
    "                band_name = single_Show.find('div', class_ = 'stevent-title').h3.text\n",
    "\n",
    "                if any([x in band_name for x in ex_array]): \n",
    "                    continue\n",
    "\n",
    "                #Start Time\n",
    "                start_Time = single_Show.find('div', class_ = 'stevent-starttime').text\n",
    "                remove_Words_Time = start_Time.replace('Starts at ', '')\n",
    "                remove_Space_Time = remove_Words_Time.replace(' ', '')\n",
    "\n",
    "                whole_Date = month_And_Day_And_Year + \" \" + remove_Space_Time\n",
    "                date = parse(whole_Date)\n",
    "                date_string = '{:%b %d, %Y %-I:%M%p}'.format(date)\n",
    "\n",
    "                showDict = {}\n",
    "                showDict['band'] = band_name\n",
    "                showDict['dateString'] = date_string\n",
    "\n",
    "                array.append(showDict)\n",
    "\n",
    "            except AttributeError as ex:\n",
    "                print('Error', ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = []\n",
    "scape_urls(url_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "#Export as JSON\n",
    "venDict = {}\n",
    "venDict['venueName'] = venue_name\n",
    "venDict['shows'] = array\n",
    "\n",
    "venue_array = [venDict]\n",
    "finalDict = {}\n",
    "finalDict['venue'] = venue_array\n",
    "\n",
    "#Save To json file\n",
    "save_path = '/Users/nathanhedgeman/Documents/Scrappers/Show Data'\n",
    "file_name = venue_name + '.json'\n",
    "complete_name = os.path.join(save_path, file_name)\n",
    "\n",
    "file = open(complete_name, 'w')\n",
    "file.write(json.dumps(finalDict, indent = 2))\n",
    "file.close()\n",
    "print(\"Complete!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('python@3.9')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
